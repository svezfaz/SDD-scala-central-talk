<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Stream Driven Development</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <script src="https://use.fontawesome.com/685eb64167.js"></script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
        <section>
          <img class="plain noalpha" src="asset/homeaway.svg">
        </section>
        <section>
          <img class="plain noalpha" src="asset/expedia.jpg">
          <aside class="notes">
          Ok letâ€™s try again. Who among you knows what Expedia is.
          -PAUSE-
          </aside>
        </section>
        <section>
          <img class="plain noalpha" src="asset/homeaway.svg">
          <img class="plain noalpha" src="asset/expedia.jpg">
          <aside class="notes">
          HomeAway is an Expedia brand focusing on the vacation rental market.
          </aside>
        </section>
        <section>
            <h1>DIGITAL MARKETING AUTOMATION</h1>
            <h3 style="visibility:hidden">Placeholder</h3>
            <p>1. automate repetitive tasks</p>
            <p>2. minimize errors and reaction times</p>
            <aside class="notes">
              HomeAway is on a quest to assert their presence as a key player in the vacation rentals market.
              And it's doing it by investing in digital marketing. What we are specifically interested in, is
              digital marketing automation.
              The end goals are two:
              the first is automating repetitive actions traditionally performed by humans in the digital marketing sphere.
              The second is to move forward and gain an edge over human activity by minimizing errors and reaction times
            </aside>
        </section>
        <section>
            <h1>STREAM DRIVEN DEVELOPMENT</h1>
            <h3>Roll your own reactive pipeline</h3>
            <p>Stefano Bonetti | Dev @ <a href="https://www.homeaway.com/info/about-us">www.homeaway.com</a></p>
            <p><i class="fa fa-github" aria-hidden="true"></i><a href="https://github.com/svezfaz"> @svezfaz</a>
            | <i class="fa fa-twitter" aria-hidden="true"></i><a href="https://twitter.com/svez_faz"> @svez_faz</a></p>
            <aside class="notes">
              My name is Stefano, I am a software engineer at HomeAway.
              and I am going to show you how we tackled marketing automation (or at least one of its many faces) developing reactive pipelines with Akka and Scala.
            </aside>
        </section>
      </section>

        <section>
          <section data-markdown>
            <script type="text/template">
              #THE PROBLEM

              Note:
              The problem is the following.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###DYNAMIC ADS
              ![dynamic ad](asset/dynamic_ad.png)

              Note:
              To make our advertisements more effective, we want them to be dynamic,
              that is, show relevant content to the relevant people at the relevant time.

              Let's assume someone already was on HomeAway.com and was looking at this property, but didn't
              book it. It could be a good target for an ad like this to try and bring him or her back to the site.

              The key point here is that whoever displays this ads needs to be up to date with our data,
              and this data can change a lot. For example the price of a property can fluctuate a lot.
              So we need to get our data to the ad platforms, and we need to get it there fast.

              This operation of sending product data to platforms is traditionally done manually by marketers,
              sending a big fat Excel file to an email address and hoping no one on the other side is on holiday.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###AUTOMATED FEEDS
              ![product feeds](asset/product_feeds_1.svg)

              Note:
              Our objective is to make this programmatic. Essentially we want to set up automated data feeds
              from us to these platforms. This poses a series of issues
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###AUTOMATED FEEDS
              ![product feeds](asset/product_feeds_2.svg)

              Note:
              First of all there are multiple platforms that we want to reach.
              These you can see are only a few of them. They want different data,
              with different presentation and they support different integrations.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###AUTOMATED FEEDS
              ![product feeds](asset/product_feeds_3.svg)

              Note:
              Secondly, the information we need to send is scattered in many places within HomeAway
              we got a variety of sources we need to integrate, and they come in different forms,
              like Kafka feeds and REST APIs
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###OBJECTIVE
              Quickly roll out pipelines which are
              1. resilient
              2. scalable

              Note:
              To tackle this, we need a pattern to enable the quick rollout of pipeline applications which are
              1) resilient - as in, they need to recover from failures and be always on
              2) scalable - to be able to handle changing and increasing load
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              In one word
              #REACTIVE

              Note:
              in other words, we want our feeds to be reactive, and indeed, reactive streams is what we went for.
            </script>
          </section>
        </section>

        <section>
          <section>
            <h1>OUR APPROACH</h1>
            <aside class="notes">Let's take a more practical look at how we tackled this problem</aside>
          </section>
          <section>
            <h3>EXAMPLE: PROPERTY FEED</h3>
             <img class="plain noalpha" src="asset/sample_pipeline.svg">
             <aside class="notes">
              We are going to explain this by example, let's assume we want to build the following reactive pipeline.
              We have a Kafka feed of property data - we want to validate this data and send it to Google Adwords.
              This is a simplified view of our real Adwords feed.
             </aside>
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/layers.svg">
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/reactive_layer.svg">
            <aside class="notes">
              We'll start by reasoning about the reactive layer, so we're going "middle-out" in terms of design.

              this layer makes the only opinionated choice in using Akka Streams.
              this layer responsibility is to use akka streams to define the stages of the pipeline
            </aside>
          </section>
          <section>
            <img class="plain noalpha" src="asset/akka_stages.svg">
            <aside class="notes">
              The whole reactive layer is based on the concept of stage.
              A stage is a reactive component of arbitrary complexity.
              It is reusable and composable.

              They come in form of sources, sinks, flows, graphs depending on the shape they have,
              that is, the number of input and output ports they have.

              It's important to note that, to us, these are not just Akka Streams DSL objects,
              but they became part of the ubiquitous language. In other words, they became the official glossary
              used within the team.
            </aside>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###Stage typing
              1. shape (strongly typed in/out ports)
              2. materialized value (last type param)

              Note:
              An Akka stage has a type signature determined by its shape, which is essentially a bunch of
              strongly typed input and output ports, and its materialized value, which can be seen as the stage
              interface, or a way to interact with the stage.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ![pipeline types](asset/pipelines_types_0.svg)

              Note:
              We found it is good practice to decide these types during our analysis phase,
              and then let them drive your testing and your implementation.
              So let's design our stage types.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ![pipeline types](asset/pipelines_types_1.svg)

              Note:
              We are going to need a source of properties, so we can design a source with output type Property.
              And our materialized value will be a Shutdownable, i.e. something we can call shutdown on, to be
              able to control our kafka stream.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ![pipeline types](asset/pipelines_types_2.svg)

              Note:
              Then we'll have a flow (1 input, 1 output) to perform the processing logic.
              Property is our input, and as output we can use Cats validated, i.e.
              something that can be invalid, i.e. a validation error, or valid, i.e. a property
              This is not the only way you can model this, just wanted to mention we like to use Cats
              types at this level to increase expressiveness.
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ![pipeline types](asset/pipelines_types_3.svg)

              Note:
              This calls for a partition of the graph, a part will handle the happy path, another the error path.
              We'll need one sink for properties, and one for errors.
              Treat errors as messages is prescribed by the reactive manifesto, and allows you to handle errors all in one place,
              so it is a very sensible thing to do
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ![pipeline types](asset/pipelines_types.svg)

              Note:
              Everything is wrapped by a graph, which is a closed stage, with no inputs or outputs.

            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###OFF-THE-SHELF STAGES
              - Akka (HTTP, TCP, ...)
              - Alpakka (AWS, SFTP, ...)
              - Reactive-Kafka
              - ...

              Note:
              Now that we have defined the stage types, we can proceed to testing and implementation.

              This is a good moment to look around for any library which might already offer what we need.
              I've listed some of them on this slide. For example if you need a source reading from Kafka..
            </script>
          </section>
          <section>
            <pre class="stretch">
              <code data-trim data-noescape class="hljs scala">
object PropertySource {
  def apply(config: KafkaConfig): Source[Property, Shutdownable] = {

    val kafkaSrc: Source[ConsumerRecord[PropertyKey, Property], Control] =
      Consumer.plainSource(settings(config),
                           Subscriptions.topics(config.topic))

    kafkaSrc
      .map(_.value)
      .mapMaterializedValue { control â‡’
        new Shutdownable {
          override def shutdown = control.shutdown()
        }
      }
  }
}
              </code>
            </pre>
            <aside class="notes">
              ...it's a good idea to use ReactiveKafka. This is a stable library that can give you
              sources and sinks to interact with Kafka. In this example you can see you can easily
              get a Kafka source by providing a bunch of settings - note that you'll need to provide
              a deserializer. However, this source is typed with reactive-kafka types. You see it is
              a source of consumerRecord that materialized to a Control.
              It is good practice to not leak these types outside of the stage, and this is what you
              can see in the second part of the source...
            </aside>
          </section>
          <section>
            <pre class="stretch">
              <code data-trim data-noescape class="hljs scala">
object ProcessingFlow {

  def apply(): Flow[Property, Validated[Error, AdWordsProperty], NotUsed] = {

    val service: PropertyProcessingService = ???

    Flow.fromFunction(service.validate)
  }
}
              </code>
            </pre>
            <aside class="notes">
              Custom stages need to be wrappers around domain logic. Flow.fromFunction can be used
              to wrap synchronous calls into flows.
            </aside>
          </section>
          <section>
            <pre class="stretch">
              <code data-trim data-noescape class="hljs scala">
object PropertySink {
  def apply(config: AdWordsConfig): Sink[AdWordsProperty, Future[Done]] = {

    val repository: PropertyRepository = ???

    Flow[AdWordsProperty]
      .mapAsync(config.parallelism)(repository.store)
      .toMat(Sink.foreach { _ â‡’
        // log / metrics
      })(Keep.right)
  }
}

object ErrorSink {
  def apply(): Sink[Error, Future[Done]] =
    Sink.foreach { error â‡’
      // log / metrics
    }
}
              </code>
            </pre>
            <aside class="notes">
              The property sink is equally simple, as it wraps another domain entity, which is a property repository.
              The difference here is that this repository will talk to Google Adwords, hence it will
              provide and async behaviour. Using the mapAsync combinator we can limit the number of parallel calls
              we'll make.
            </aside>
          </section>
          <section>
            <pre class="stretch">
              <code data-trim data-noescape class="hljs scala">
object Graph {
  def apply(source   : Source[Property, Shutdownable],
            flow     : Flow[Property, Validated[Error, AdWordsProperty], NotUsed],
            errorSink: Sink[Error, Future[Done]],
            sink     : Sink[AdWordsProperty, Future[Done]]):
  RunnableGraph[(Shutdownable, Future[Done])] = {

    RunnableGraph.fromGraph(GraphDSL.create(source, sink)
    ((_, _)) { implicit builder â‡’ (source, sink) â‡’

      val p = builder.add(PartitionValidated.apply()[Error, AdWordsProperty])

                        p.out0 ~> errorSink
      source ~> flow ~> p.in
                        p.out1 ~> sink

      ClosedShape
    })
  }
}
              </code>
            </pre>
            <aside class="notes">
              The graph is where everything comes together. Akka Streams graphDSL allows to define
              your topology in a very visual way.

              Using GraphDSL might seem verbose as it carries some boilerplate - and for this example is definitely overkill.
              However, the graphDSL will reflect the topology of your pipeline while it grows.
              It is self documenting code, which is awesome.

              You can clearly see how the graph resembles the graph we initially designed on paper.
            </aside>
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/domain_layer.svg">
            <aside class="notes">
              It is where your business logic lives. This layer must not know about Akka or streaming
            </aside>
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/domain.svg">
            <aside class="notes">
              The stuff we have in here has been driven by the decisions we have made on the reactive layer.
              We need entities representing properties, and a validation error, which we can now model,
              we need a service to validate, and we need a repository to deal with the AdWords API.
            </aside>
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/application_layer.svg">
          </section>
          <section data-markdown>
            <script type="text/template">
              ###APPLICATION LAYER
              1. config management
              2. stages creation
              3. lifecycle management
              4. docker packaging

              Note:
              Application layer responsibility:
              1) manage configuration
              2) create the stages and the graph
              3) Start and stop the streaming of data and restarting the stream in case of failure
              4) Enable Docker packaging
            </script>
          </section>
          <section>
            <img class="stretch plain noalpha" src="asset/layers_scale.svg">
            <aside class="notes">
              Docker packaging allows for greater scalability
              as we can deploy as many feed instances as we want in the cloud.
            </aside>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
              ###WE WANTED OUR FEEDS TO BE

              Note:
              To bring us back to why are we doing all this
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###WE WANTED OUR FEEDS TO BE
              1. resilient

              Note:
              - Reactive streams enable backpressure so the feeds cannot be overloaded,
              this makes memory errors are a thing of the past
              - Any unpredicted failure results in restarting the stream
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ###WE WANTED OUR FEEDS TO BE
              1. resilient
              2. scalable

              Note:
              Using reactive streams we squeeze our local environments by minimizing resource contention
              (as a result of asynchronous non-blocking processing)
              Whenever this is not sufficient, we can easily scale horizontally thanks to Kafka and Docker.
            </script>
          </section>
        </section>

        <section>
            <h1>THANK YOU!</h1>
            <p>Stefano Bonetti | Dev @ <a href="https://www.homeaway.com/info/about-us">www.homeaway.com</a></p>
            <p><i class="fa fa-github" aria-hidden="true"></i><a href="https://github.com/svezfaz"> @svezfaz</a>
            | <i class="fa fa-twitter" aria-hidden="true"></i><a href="https://twitter.com/svez_faz"> @svez_faz</a></p>
        </section>
      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        history: true,
        transition: 'none',
        showNotes: false,

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });

    </script>
  </body>
</html>
